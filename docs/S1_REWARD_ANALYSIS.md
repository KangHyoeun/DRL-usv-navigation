## ðŸ“Š S1 Scenario Reward Function Analysis

### ðŸŽ¯ **Current Reward Function**

```python
if goal:
    return 10000.0          # Goal bonus
elif collision:
    return -10000.0         # Collision penalty

# Step rewards:
progress_reward = (prev_distance - distance) Ã— 100
velocity_reward = action[0] Ã— 5
obstacle_penalty = -(5.0 - min_dist) Ã— 10 if min_dist < 5m else 0
step_penalty = -1

total = progress + velocity + obstacle + step
```

---

### ðŸ“ˆ **Expected Reward for S1 (Straight Line)**

**Optimal trajectory (60ì´ˆ, ì§ì§„):**
```
Progress reward:  180m Ã— 100 = 18,000
Goal bonus:                    10,000
Velocity bonus:   3.0 Ã— 5 Ã— 600 steps = 9,000
Step penalty:     -1 Ã— 600 = -600
Obstacle penalty: 0 (no obstacles in s1)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:            ~36,400 âœ“
```

**Per-step breakdown:**
- Good step: +30 to +50 (progress + velocity - step)
- Bad step: -1 to -10 (no progress)

---

### âš ï¸ **ë¬¸ì œì  & ê°œì„ ì‚¬í•­**

#### **1. Phase 1 Action Interval (í˜„ìž¬: 1.0s)**
```python
action_dt = 1.0  # DRL action update every 1.0s
```

**ë¬¸ì œ:**
- ì§ì§„ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ 1.0ì´ˆë§ˆë‹¤ action ì—…ë°ì´íŠ¸ëŠ” ë„ˆë¬´ ëŠë¦¼
- ë¯¸ì„¸í•œ heading ì¡°ì •ì´ ì–´ë ¤ì›€

**ê¶Œìž¥:**
```python
action_dt = 0.5  # 0.5ì´ˆ (5 physics steps)
# ë˜ëŠ”
action_dt = 0.3  # 0.3ì´ˆ (3 physics steps)
```

**ì´ìœ :**
- Otter USV controller settling time: ~3ì´ˆ
- í•˜ì§€ë§Œ ì§ì§„ì€ complex maneuveringì´ ì•„ë‹ˆë¯€ë¡œ ë” ë¹ ë¥¸ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
- 0.5ì´ˆë©´ ì¶©ë¶„í•œ settling + ë¹ ë¥¸ í•™ìŠµ

---

#### **2. Obstacle Penalty Threshold (í˜„ìž¬: 5m)**
```python
if min_distance < 5.0:
    obstacle_penalty = -(5.0 - min_distance) Ã— 10
```

**ë¬¸ì œ:**
- S1 í™˜ê²½ì˜ ë²½ê¹Œì§€ ê±°ë¦¬: **50m (ì¢Œìš°), 90-100m (ì „í›„)**
- 5m thresholdëŠ” **ë„ˆë¬´ ìž‘ì•„ì„œ ë²½ íšŒí”¼ í•™ìŠµ ë¶ˆê°€**

**ê¶Œìž¥ (Phaseë³„ ì¡°ì •):**

**Phase 1 (ì§ì§„ í•™ìŠµ):**
```python
# ë²½ íšŒí”¼ëŠ” ë‚˜ì¤‘ì— í•™ìŠµ
if min_distance < 3.0:  # ë§¤ìš° ê°€ê¹Œìš¸ ë•Œë§Œ
    obstacle_penalty = -(3.0 - min_distance) Ã— 20
```

**Phase 2 (ìž¥ì• ë¬¼ íšŒí”¼ í•™ìŠµ ì‹œ):**
```python
# ë” ë¨¼ ê±°ë¦¬ì—ì„œ íšŒí”¼ ì‹œìž‘
if min_distance < 15.0:
    obstacle_penalty = -(15.0 - min_distance) Ã— 5
elif min_distance < 5.0:
    obstacle_penalty = -(5.0 - min_distance) Ã— 20  # ë§¤ìš° ê°€ê¹Œìš°ë©´ ê°•í•œ penalty
```

---

#### **3. Progress Reward Scale**
```python
progress_reward = (prev_distance - distance) Ã— 100
```

**ë¶„ì„:**
- 1m ì§„í–‰ â†’ +100 reward
- 0.1m ì§„í–‰ â†’ +10 reward (typical per step at 3m/s)

**íŒë‹¨:** âœ… **ì ì ˆí•¨!**
- velocity reward (15) + progress (10) - step (-1) = +24 per step
- ëˆ„ì í•˜ë©´ goal bonus í¬í•¨ ~36,000

**ë‹¨, ê³ ë ¤ì‚¬í•­:**
- ë„ˆë¬´ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ë©´ exploration ë¶€ì¡± ê°€ëŠ¥
- ì´ˆë°˜ì—ëŠ” reward ìŠ¤ì¼€ì¼ ë‚®ì¶”ëŠ” ê²ƒë„ ë°©ë²•

---

#### **4. Step Penalty**
```python
step_penalty = -1.0
```

**ë¶„ì„:**
- ë§¤ stepë§ˆë‹¤ -1
- ë¹ ë¥´ê²Œ goal ë„ë‹¬í•˜ë„ë¡ ìœ ë„

**íŒë‹¨:** âœ… **ì ì ˆí•¨!**
- Progress reward (10-30)ê°€ ì¶©ë¶„ížˆ í¬ë¯€ë¡œ -1ì€ ë¯¸ë¯¸
- ì‹œê°„ íš¨ìœ¨ì„± ê°•ì¡°

---

### ðŸ”§ **ê¶Œìž¥ ìˆ˜ì •ì‚¬í•­**

#### **ìˆ˜ì • 1: action_dt ì¤„ì´ê¸°**
```python
# otter_sim.py line 42
self.action_dt = 0.5  # 1.0 â†’ 0.5ì´ˆë¡œ ë³€ê²½
```

#### **ìˆ˜ì • 2: obstacle threshold ì¡°ì •**
```python
# otter_sim.py get_reward í•¨ìˆ˜
# 3. Obstacle penalty (WALL AVOIDANCE)
min_distance = min(laser_scan)
if min_distance < 3.0:  # 5.0 â†’ 3.0ìœ¼ë¡œ ë³€ê²½
    obstacle_penalty = -(3.0 - min_distance) Ã— 20  # 10 â†’ 20ìœ¼ë¡œ ê°•í™”
else:
    obstacle_penalty = 0.0
```

#### **ìˆ˜ì • 3 (ì„ íƒ): Progress reward smoothing**
```python
# Phase 1: ì§ì§„ í•™ìŠµìš© (ì¢€ ë” ë³´ìˆ˜ì )
progress_reward = (prev_distance - distance) Ã— 50  # 100 â†’ 50

# Phase 2: íšŒí”¼ í•™ìŠµ í›„ ì›ë³µ
progress_reward = (prev_distance - distance) Ã— 100
```

---

### ðŸ“‹ **ìµœì¢… ê¶Œìž¥ Reward Function (S1ìš©)**

```python
@staticmethod
def get_reward(goal, collision, action, laser_scan, distance, cos, prev_distance):
    # Terminal rewards
    if goal:
        return 10000.0
    elif collision:
        return -10000.0
    
    # Step rewards
    # 1. Progress (MAIN)
    progress_reward = 0.0
    if prev_distance is not None:
        progress = prev_distance - distance
        progress_reward = progress * 100.0  # Keep original scale
    
    # 2. Velocity bonus
    velocity_reward = action[0] * 5.0
    
    # 3. Obstacle penalty (ë²½ íšŒí”¼ - threshold ë‚®ì¶¤)
    min_distance = min(laser_scan)
    if min_distance < 3.0:  # 5.0 â†’ 3.0
        obstacle_penalty = -(3.0 - min_distance) * 20.0  # 10 â†’ 20
    else:
        obstacle_penalty = 0.0
    
    # 4. Heading alignment bonus (ì§ì§„ ê°•í™”)
    heading_bonus = cos * 2.0  # ì •ë ¬ë˜ë©´ +2, ë°˜ëŒ€ë©´ -2
    
    # 5. Step penalty
    step_penalty = -1.0
    
    # Total
    total_reward = (progress_reward + velocity_reward + 
                   obstacle_penalty + heading_bonus + step_penalty)
    
    return total_reward
```

**ë³€ê²½ì‚¬í•­:**
1. âœ… Obstacle threshold: 5m â†’ 3m (ë²½ì´ ë©€ì–´ì„œ)
2. âœ… Obstacle penalty weight: 10 â†’ 20 (ë” ê°•í•˜ê²Œ)
3. âœ… **Heading bonus ì¶”ê°€**: ì§ì§„ ìœ ë„ (+2 when aligned)

---

### ðŸŽ¯ **Expected Performance (Revised)**

**Optimal trajectory (60ì´ˆ):**
```
Progress:  18,000
Goal:      10,000
Velocity:   9,000
Heading:    1,200  (cos â‰ˆ 1.0 most of time)
Step:        -600
Obstacle:      0   (no close walls)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:    ~37,600 âœ“
```

**Per-step average:** +62 (good learning signal!)

---

### ðŸš€ **Action Plan**

1. **Phase 1 (ì§€ê¸ˆ):** ì§ì§„ í•™ìŠµ
   - action_dt = 0.5s
   - Obstacle threshold = 3m
   - Heading bonus ì¶”ê°€

2. **Phase 2 (ë‚˜ì¤‘):** íšŒí”¼ í•™ìŠµ
   - action_dt = 1.0s (ë³µìž¡í•œ maneuvering)
   - Obstacle threshold = 10-15m
   - Multiple obstacles ì¶”ê°€

3. **Phase 3 (ìµœì¢…):** COLREGs í•™ìŠµ
   - ë™ì  ìž¥ì• ë¬¼
   - ì¡°ìš° ìƒí™©ë³„ reward shaping

---

ìˆ˜ì •í• ê¹Œ? ì•„ë‹ˆë©´ ì¼ë‹¨ í˜„ìž¬ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ ì‹œìž‘í•´ë³¼ê¹Œ?
